# Visual Sycophancy in Multimodal Models

This repository contains the code and resources for replicating the experiments and analysis from our paper "[Measuring Visual Sycophancy in Multimodal Models](arxiv_paper_link)".

## Table of Contents
1. [Introduction](#introduction)
2. [Repository Structure](#repository-structure)
3. [Setup](#setup)
4. [Generating Experiments](#generating-experiments)
5. [Evaluating Models](#evaluating-models)
6. [Analyzing Results](#analyzing-results)
7. [Models Tested](#models-tested)
8. [Benchmarks](#benchmarks)
9. [Results](#results)
10. [Contributing](#contributing)
11. [License](#license)

## Introduction

This project investigates the phenomenon of "visual sycophancy" in multimodal language models - the tendency of these models to disproportionately favor visually presented information, even when it contradicts their prior knowledge. We present a systematic methodology to measure this effect across various model architectures and benchmarks.

## Repository Structure

- `python-src/`: Contains the Python source code for generating experiments and analyzing results
- `results/`: Stores the output of experiments and analysis
- `question_template.html`: HTML template for rendering questions
- `full_results.rtf`: Comprehensive results in Rich Text Format

## Setup

[Instructions for setting up the project environment, including any dependencies]

## Generating Experiments

To generate the experimental prompts:

1. For vMMLU:
